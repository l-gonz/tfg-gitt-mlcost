...............*..*
optimization finished, #iter = 17922
obj = -16727.568031, rho = 0.332034
nSV = 17613, nBSV = 17184
Total nSV = 17613
---------------------------
Running on mltest-low
Ubuntu 20.04.6 LTS x86_64
Python 3.12.3
Intel(R) Xeon(R) Platinum 8272CL CPU @ 2.60GHz
Memory: 894.10 MB
Start load: 97.00%
---------------------------
DATA PREPROCESSING SUMMARY
Original data: 2.584e+06 bytes


Discarded features: 0
Discarded rows for missing labels: 0
Trained numerical features: ['date', 'period', 'nswprice', 'nswdemand', 'vicprice', 'vicdemand', 'transfer']
Trained categorical features: ['day']

Removed rows from missing categorical values - Train: 0 , Test: 0
Final train set rows: 36249, test set rows: 9063

Target distribution:
         train      test
class                   
DOWN   0.57403  0.581154
UP     0.42597  0.418846
---------------------------
Linear
test_accuracy: 0.75
test_precision: 0.77
test_f1_score: 0.76
test_recall: 0.77
n_samples: 45312
fit_time: 0.09
Time: 94 ms
Emissions: 215.38 μg (CO2-equivalents)
Energy consumed: 1.11 mWh
---------------------------
Forest
test_accuracy: 0.90
test_precision: 0.91
test_f1_score: 0.91
test_recall: 0.91
n_samples: 45312
fit_time: 5.99
Time: 5.99 s
Emissions: 13.74 mg (CO2-equivalents)
Energy consumed: 70.87 mWh
---------------------------
[LibSVM]SupportVector
test_accuracy: 0.79
test_precision: 0.81
test_f1_score: 0.80
test_recall: 0.81
n_samples: 45312
fit_time: 38.89
Time: 38.89 s
Emissions: 89.36 mg (CO2-equivalents)
Energy consumed: 460.96 mWh
---------------------------
Neighbors
test_accuracy: 0.83
test_precision: 0.83
test_f1_score: 0.83
test_recall: 0.83
n_samples: 45312
fit_time: 1.10
Time: 1.095 s
Emissions: 2.52 mg (CO2-equivalents)
Energy consumed: 12.98 mWh
---------------------------
NaiveBayes
test_accuracy: 0.70
test_precision: 0.78
test_f1_score: 0.72
test_recall: 0.74
n_samples: 45312
fit_time: 0.05
Time: 48.86 ms
Emissions: 112.15 μg (CO2-equivalents)
Energy consumed: 578.50 μWh
---------------------------
      Iter       Train Loss   Remaining Time 
         1           1.3018            6.36s
         2           1.2512            6.21s
         3           1.2102            6.09s
         4           1.1746            6.03s
         5           1.1414            5.97s
         6           1.1146            5.91s
         7           1.0927            5.83s
         8           1.0710            5.77s
         9           1.0526            5.70s
        10           1.0380            5.63s
        20           0.9363            4.98s
        30           0.8832            4.35s
        40           0.8467            3.73s
        50           0.8231            3.10s
        60           0.8044            2.48s
        70           0.7872            1.86s
        80           0.7725            1.24s
        90           0.7621            0.62s
       100           0.7510            0.00s
GradientBoost
test_accuracy: 0.83
test_precision: 0.84
test_f1_score: 0.84
test_recall: 0.84
n_samples: 45312
fit_time: 6.23
Time: 6.231 s
Emissions: 14.31 mg (CO2-equivalents)
Energy consumed: 73.84 mWh
---------------------------
Iteration 1, loss = 0.55167786
Iteration 2, loss = 0.47600896
Iteration 3, loss = 0.46543563
Iteration 4, loss = 0.45965118
Iteration 5, loss = 0.45619904
Iteration 6, loss = 0.45315026
Iteration 7, loss = 0.45101206
Iteration 8, loss = 0.44881862
Iteration 9, loss = 0.44699109
Iteration 10, loss = 0.44516350
Iteration 11, loss = 0.44378628
Iteration 12, loss = 0.44183523
Iteration 13, loss = 0.44016298
Iteration 14, loss = 0.43830324
Iteration 15, loss = 0.43734697
Iteration 16, loss = 0.43504708
Iteration 17, loss = 0.43396125
Iteration 18, loss = 0.43224956
Iteration 19, loss = 0.43078362
Iteration 20, loss = 0.42950186
Iteration 21, loss = 0.42800833
Iteration 22, loss = 0.42719377
Iteration 23, loss = 0.42549173
Iteration 24, loss = 0.42430963
Iteration 25, loss = 0.42289202
Iteration 26, loss = 0.42175966
Iteration 27, loss = 0.42138402
Iteration 28, loss = 0.42009965
Iteration 29, loss = 0.41902939
Iteration 30, loss = 0.41812843
Iteration 31, loss = 0.41697843
Iteration 32, loss = 0.41664828
Iteration 33, loss = 0.41526422
Iteration 34, loss = 0.41515958
Iteration 35, loss = 0.41478768
Iteration 36, loss = 0.41342090
Iteration 37, loss = 0.41235988
Iteration 38, loss = 0.41181002
Iteration 39, loss = 0.41128571
Iteration 40, loss = 0.41093120
Iteration 41, loss = 0.40964553
Iteration 42, loss = 0.40941116
Iteration 43, loss = 0.40878428
Iteration 44, loss = 0.40793274
Iteration 45, loss = 0.40750346
Iteration 46, loss = 0.40716817
Iteration 47, loss = 0.40646083
Iteration 48, loss = 0.40619701
Iteration 49, loss = 0.40558798
Iteration 50, loss = 0.40492952
Iteration 51, loss = 0.40431124
Iteration 52, loss = 0.40381460
Iteration 53, loss = 0.40317506
Iteration 54, loss = 0.40289263
Iteration 55, loss = 0.40249529
Iteration 56, loss = 0.40217974
Iteration 57, loss = 0.40178697
Iteration 58, loss = 0.40051837
Iteration 59, loss = 0.40094929
Iteration 60, loss = 0.40054150
Iteration 61, loss = 0.39952949
Iteration 62, loss = 0.39933119
Iteration 63, loss = 0.39916692
Iteration 64, loss = 0.39874469
Iteration 65, loss = 0.39767764
Iteration 66, loss = 0.39734074
Iteration 67, loss = 0.39692369
Iteration 68, loss = 0.39705347
Iteration 69, loss = 0.39639236
Iteration 70, loss = 0.39542024
Iteration 71, loss = 0.39552279
Iteration 72, loss = 0.39516800
Iteration 73, loss = 0.39530548
Iteration 74, loss = 0.39441342
Iteration 75, loss = 0.39378156
Iteration 76, loss = 0.39379164
Iteration 77, loss = 0.39326666
Iteration 78, loss = 0.39285537
Iteration 79, loss = 0.39232388
Iteration 80, loss = 0.39191068
Iteration 81, loss = 0.39136792
Iteration 82, loss = 0.39183745
Iteration 83, loss = 0.39105096
Iteration 84, loss = 0.39079190
Iteration 85, loss = 0.39008348
Iteration 86, loss = 0.39001118
Iteration 87, loss = 0.38904381
Iteration 88, loss = 0.38929946
Iteration 89, loss = 0.38916323
Iteration 90, loss = 0.38894806
Iteration 91, loss = 0.38921713
Iteration 92, loss = 0.38857508
Iteration 93, loss = 0.38762440
Iteration 94, loss = 0.38746762
Iteration 95, loss = 0.38731663
Iteration 96, loss = 0.38684202
Iteration 97, loss = 0.38636567
Iteration 98, loss = 0.38644537
Iteration 99, loss = 0.38640065
Iteration 100, loss = 0.38557592
Iteration 101, loss = 0.38524962
Iteration 102, loss = 0.38526906
Iteration 103, loss = 0.38474890
Iteration 104, loss = 0.38502534
Iteration 105, loss = 0.38521850
Iteration 106, loss = 0.38398439
Iteration 107, loss = 0.38411213
Iteration 108, loss = 0.38372849
Iteration 109, loss = 0.38386979
Iteration 110, loss = 0.38313866
Iteration 111, loss = 0.38294381
Iteration 112, loss = 0.38283751
Iteration 113, loss = 0.38248475
Iteration 114, loss = 0.38240607
Iteration 115, loss = 0.38166190
Iteration 116, loss = 0.38223821
Iteration 117, loss = 0.38152283
Iteration 118, loss = 0.38128536
Iteration 119, loss = 0.38150820
Iteration 120, loss = 0.38117063
Iteration 121, loss = 0.38060308
Iteration 122, loss = 0.38107728
Iteration 123, loss = 0.38057015
Iteration 124, loss = 0.38019724
Iteration 125, loss = 0.37961897
Iteration 126, loss = 0.37980286
Iteration 127, loss = 0.37948373
Iteration 128, loss = 0.37928129
Iteration 129, loss = 0.37901874
Iteration 130, loss = 0.37907290
Iteration 131, loss = 0.37869826
Iteration 132, loss = 0.37842704
Iteration 133, loss = 0.37824908
Iteration 134, loss = 0.37830293
Iteration 135, loss = 0.37822771
Iteration 136, loss = 0.37762666
Iteration 137, loss = 0.37696175
Iteration 138, loss = 0.37783817
Iteration 139, loss = 0.37718346
Iteration 140, loss = 0.37780322
Iteration 141, loss = 0.37711628
Iteration 142, loss = 0.37672009
Iteration 143, loss = 0.37660003
Iteration 144, loss = 0.37637510
Iteration 145, loss = 0.37622288
Iteration 146, loss = 0.37607109
Iteration 147, loss = 0.37598370
Iteration 148, loss = 0.37579678
Iteration 149, loss = 0.37606405
Iteration 150, loss = 0.37662314
Iteration 151, loss = 0.37490138
Iteration 152, loss = 0.37523114
Iteration 153, loss = 0.37490403
Iteration 154, loss = 0.37471053
Iteration 155, loss = 0.37424196
Iteration 156, loss = 0.37505670
Iteration 157, loss = 0.37421133
Iteration 158, loss = 0.37563299
Iteration 159, loss = 0.37417035
Iteration 160, loss = 0.37443040
Iteration 161, loss = 0.37387814
Iteration 162, loss = 0.37336974
Iteration 163, loss = 0.37427099
Iteration 164, loss = 0.37372410
Iteration 165, loss = 0.37326212
Iteration 166, loss = 0.37311715
Iteration 167, loss = 0.37320584
Iteration 168, loss = 0.37344458
Iteration 169, loss = 0.37262984
Iteration 170, loss = 0.37193334
Iteration 171, loss = 0.37229619
Iteration 172, loss = 0.37213763
Iteration 173, loss = 0.37224342
Iteration 174, loss = 0.37278960
Iteration 175, loss = 0.37173910
Iteration 176, loss = 0.37149222
Iteration 177, loss = 0.37071140
Iteration 178, loss = 0.37080220
Iteration 179, loss = 0.37124828
Iteration 180, loss = 0.37148021
Iteration 181, loss = 0.37150484
Iteration 182, loss = 0.37089872
Iteration 183, loss = 0.37105118
Iteration 184, loss = 0.37097954
Iteration 185, loss = 0.37077015
Iteration 186, loss = 0.37057242
Iteration 187, loss = 0.37080105
Iteration 188, loss = 0.37026644
Iteration 189, loss = 0.36982662
Iteration 190, loss = 0.36989612
Iteration 191, loss = 0.36929494
Iteration 192, loss = 0.36945103
Iteration 193, loss = 0.36937800
Iteration 194, loss = 0.36968575
Iteration 195, loss = 0.36945347
Iteration 196, loss = 0.36876246
Iteration 197, loss = 0.36953502
Iteration 198, loss = 0.36852260
Iteration 199, loss = 0.36907403
Iteration 200, loss = 0.36906267
Iteration 201, loss = 0.36902502
Iteration 202, loss = 0.36850710
Iteration 203, loss = 0.36820781
Iteration 204, loss = 0.36786300
Iteration 205, loss = 0.36781801
Iteration 206, loss = 0.36910906
Iteration 207, loss = 0.36862971
Iteration 208, loss = 0.36834613
Iteration 209, loss = 0.36797225
Iteration 210, loss = 0.36768749
Iteration 211, loss = 0.36729396
Iteration 212, loss = 0.36718252
Iteration 213, loss = 0.36764185
Iteration 214, loss = 0.36741654
Iteration 215, loss = 0.36710065
Iteration 216, loss = 0.36717292
Iteration 217, loss = 0.36654196
Iteration 218, loss = 0.36670889
Iteration 219, loss = 0.36733013
Iteration 220, loss = 0.36661677
Iteration 221, loss = 0.36689423
Iteration 222, loss = 0.36647302
Iteration 223, loss = 0.36577670
Iteration 224, loss = 0.36602046
Iteration 225, loss = 0.36552196
Iteration 226, loss = 0.36618857
Iteration 227, loss = 0.36673760
Iteration 228, loss = 0.36639420
Iteration 229, loss = 0.36600004
Iteration 230, loss = 0.36518385
Iteration 231, loss = 0.36513119
Iteration 232, loss = 0.36487999
Iteration 233, loss = 0.36490638
Iteration 234, loss = 0.36493929
Iteration 235, loss = 0.36457045
Iteration 236, loss = 0.36537890
Iteration 237, loss = 0.36430662
Iteration 238, loss = 0.36563531
Iteration 239, loss = 0.36478515
Iteration 240, loss = 0.36457305
Iteration 241, loss = 0.36421812
Iteration 242, loss = 0.36425542
Iteration 243, loss = 0.36430658
Iteration 244, loss = 0.36421936
Iteration 245, loss = 0.36373584
Iteration 246, loss = 0.36411461
Iteration 247, loss = 0.36375587
Iteration 248, loss = 0.36358473
Iteration 249, loss = 0.36374728
Iteration 250, loss = 0.36428648
Iteration 251, loss = 0.36357670
Iteration 252, loss = 0.36413840
Iteration 253, loss = 0.36312148
Iteration 254, loss = 0.36301066
Iteration 255, loss = 0.36345832
Iteration 256, loss = 0.36327225
Iteration 257, loss = 0.36287369
Iteration 258, loss = 0.36257460
Iteration 259, loss = 0.36218200
Iteration 260, loss = 0.36229844
Iteration 261, loss = 0.36290536
Iteration 262, loss = 0.36307669
Iteration 263, loss = 0.36241794
Iteration 264, loss = 0.36392657
Iteration 265, loss = 0.36186685
Iteration 266, loss = 0.36269859
Iteration 267, loss = 0.36263684
Iteration 268, loss = 0.36209974
Iteration 269, loss = 0.36204297
Iteration 270, loss = 0.36242265
Iteration 271, loss = 0.36202402
Iteration 272, loss = 0.36194149
Iteration 273, loss = 0.36115139
Iteration 274, loss = 0.36190790
Iteration 275, loss = 0.36106958
Iteration 276, loss = 0.36209948
Iteration 277, loss = 0.36125816
Iteration 278, loss = 0.36156565
Iteration 279, loss = 0.36135654
Iteration 280, loss = 0.36108769
Iteration 281, loss = 0.36106913
Iteration 282, loss = 0.36090117
Iteration 283, loss = 0.36125575
Iteration 284, loss = 0.36033302
Iteration 285, loss = 0.36067297
Iteration 286, loss = 0.36048866
Iteration 287, loss = 0.36081892
Iteration 288, loss = 0.36029124
Iteration 289, loss = 0.36024111
Iteration 290, loss = 0.36073300
Iteration 291, loss = 0.36018506
Iteration 292, loss = 0.36043416
Iteration 293, loss = 0.36090883
Iteration 294, loss = 0.36002935
Iteration 295, loss = 0.35942508
Iteration 296, loss = 0.35941866
Iteration 297, loss = 0.35999019
Iteration 298, loss = 0.35945997
Iteration 299, loss = 0.35983587
Iteration 300, loss = 0.35893785
Iteration 301, loss = 0.35963244
Iteration 302, loss = 0.35907456
Iteration 303, loss = 0.35969215
Iteration 304, loss = 0.35935814
Iteration 305, loss = 0.35907436
Iteration 306, loss = 0.35940510
Iteration 307, loss = 0.35904440
Iteration 308, loss = 0.35893762
Iteration 309, loss = 0.35868981
Iteration 310, loss = 0.35873918
Iteration 311, loss = 0.35921474
Iteration 312, loss = 0.35900781
Iteration 313, loss = 0.35894014
Iteration 314, loss = 0.35911882
Iteration 315, loss = 0.35838813
Iteration 316, loss = 0.35866569
Iteration 317, loss = 0.35761125
Iteration 318, loss = 0.35932644
Iteration 319, loss = 0.35766335
Iteration 320, loss = 0.35823760
Iteration 321, loss = 0.35851722
Iteration 322, loss = 0.35905094
Iteration 323, loss = 0.35848294
Iteration 324, loss = 0.35745873
Iteration 325, loss = 0.35749203
Iteration 326, loss = 0.35761891
Iteration 327, loss = 0.35738817
Iteration 328, loss = 0.35822337
Iteration 329, loss = 0.35762468
Iteration 330, loss = 0.35730798
Iteration 331, loss = 0.35722828
Iteration 332, loss = 0.35710175
Iteration 333, loss = 0.35753106
Iteration 334, loss = 0.35694058
Iteration 335, loss = 0.35659492
Iteration 336, loss = 0.35753727
Iteration 337, loss = 0.35685363
Iteration 338, loss = 0.35659465
Iteration 339, loss = 0.35756408
Iteration 340, loss = 0.35626049
Iteration 341, loss = 0.35671215
Iteration 342, loss = 0.35663561
Iteration 343, loss = 0.35615303
Iteration 344, loss = 0.35734065
Iteration 345, loss = 0.35645757
Iteration 346, loss = 0.35645038
Iteration 347, loss = 0.35657061
Iteration 348, loss = 0.35633560
Iteration 349, loss = 0.35561642
Iteration 350, loss = 0.35622145
Iteration 351, loss = 0.35569999
Iteration 352, loss = 0.35619683
Iteration 353, loss = 0.35614993
Iteration 354, loss = 0.35544228
Iteration 355, loss = 0.35510804
Iteration 356, loss = 0.35594061
Iteration 357, loss = 0.35600690
Iteration 358, loss = 0.35506175
Iteration 359, loss = 0.35485150
Iteration 360, loss = 0.35535623
Iteration 361, loss = 0.35525013
Iteration 362, loss = 0.35528195
Iteration 363, loss = 0.35543905
Iteration 364, loss = 0.35558725
Iteration 365, loss = 0.35561624
Iteration 366, loss = 0.35439126
Iteration 367, loss = 0.35527078
Iteration 368, loss = 0.35534567
Iteration 369, loss = 0.35461632
Iteration 370, loss = 0.35552853
Iteration 371, loss = 0.35474637
Iteration 372, loss = 0.35483348
Iteration 373, loss = 0.35452959
Iteration 374, loss = 0.35450086
Iteration 375, loss = 0.35438770
Iteration 376, loss = 0.35493465
Iteration 377, loss = 0.35422661
Iteration 378, loss = 0.35557106
Iteration 379, loss = 0.35417227
Iteration 380, loss = 0.35496236
Iteration 381, loss = 0.35429611
Iteration 382, loss = 0.35346299
Iteration 383, loss = 0.35423952
Iteration 384, loss = 0.35389903
Iteration 385, loss = 0.35409751
Iteration 386, loss = 0.35369737
Iteration 387, loss = 0.35353022
Iteration 388, loss = 0.35424341
Iteration 389, loss = 0.35426049
Iteration 390, loss = 0.35457729
Iteration 391, loss = 0.35416747
Iteration 392, loss = 0.35355447
Iteration 393, loss = 0.35408701
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Neural
test_accuracy: 0.82
test_precision: 0.83
test_f1_score: 0.83
test_recall: 0.83
n_samples: 45312
fit_time: 43.11
Time: 43.11 s
Emissions: 99.03 mg (CO2-equivalents)
Energy consumed: 510.80 mWh
---------------------------
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =           15     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  6.93147D-01    |proj g|=  1.87275D-01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
   15     16     19      1     0     0   3.155D-05   5.005D-01
  F =  0.50052170532253049     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
