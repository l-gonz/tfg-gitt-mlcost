---------------------------
Running on mltest-low
Ubuntu 20.04.6 LTS x86_64
Python 3.12.3
Intel(R) Xeon(R) Platinum 8272CL CPU @ 2.60GHz
Memory: 894.10 MB
Start load: 59.00%
---------------------------
DATA PREPROCESSING SUMMARY
Original data: 8.200e+07 bytes


Discarded features: 0
Discarded rows for missing labels: 0
Trained numerical features: ['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10']
Trained categorical features: []

Removed rows from missing categorical values - Train: 0 , Test: 0
Final train set rows: 820007, test set rows: 205002

Target distribution:
          train      test
Class                    
1      0.501159  0.501200
2      0.422578  0.422337
3      0.047676  0.047478
4      0.021096  0.021146
5      0.003822  0.004117
6      0.001991  0.002034
7      0.001422  0.001434
8      0.000228  0.000239
9      0.000020  0.000005
10     0.000007  0.000010
---------------------------
Neighbors
test_accuracy: 0.19
test_precision: 0.51
test_f1_score: 0.52
test_recall: 0.54
n_samples: 1025009
fit_time: 122.09
Time: 2 min 2.088 s
Emissions: 280.65 mg (CO2-equivalents)
Energy consumed: 1.45 Wh
---------------------------
---------------------------
Running on mltest-low
Ubuntu 20.04.6 LTS x86_64
Python 3.12.3
Intel(R) Xeon(R) Platinum 8272CL CPU @ 2.60GHz
Memory: 894.10 MB
Start load: 105.00%
---------------------------
DATA PREPROCESSING SUMMARY
Original data: 8.200e+07 bytes


Discarded features: 0
Discarded rows for missing labels: 0
Trained numerical features: ['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10']
Trained categorical features: []

Removed rows from missing categorical values - Train: 0 , Test: 0
Final train set rows: 820007, test set rows: 205002

Target distribution:
          train      test
Class                    
1      0.501159  0.501200
2      0.422578  0.422337
3      0.047676  0.047478
4      0.021096  0.021146
5      0.003822  0.004117
6      0.001991  0.002034
7      0.001422  0.001434
8      0.000228  0.000239
9      0.000020  0.000005
10     0.000007  0.000010
---------------------------
NaiveBayes
test_accuracy: 0.10
test_precision: 0.25
test_f1_score: 0.33
test_recall: 0.50
n_samples: 1025009
fit_time: 1.29
Time: 1.291 s
Emissions: 2.97 mg (CO2-equivalents)
Energy consumed: 15.30 mWh
---------------------------
---------------------------
Running on mltest-low
Ubuntu 20.04.6 LTS x86_64
Python 3.12.3
Intel(R) Xeon(R) Platinum 8272CL CPU @ 2.60GHz
Memory: 894.10 MB
Start load: 128.00%
---------------------------
DATA PREPROCESSING SUMMARY
Original data: 8.200e+07 bytes


Discarded features: 0
Discarded rows for missing labels: 0
Trained numerical features: ['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10']
Trained categorical features: []

Removed rows from missing categorical values - Train: 0 , Test: 0
Final train set rows: 820007, test set rows: 205002

Target distribution:
          train      test
Class                    
1      0.501159  0.501200
2      0.422578  0.422337
3      0.047676  0.047478
4      0.021096  0.021146
5      0.003822  0.004117
6      0.001991  0.002034
7      0.001422  0.001434
8      0.000228  0.000239
9      0.000020  0.000005
10     0.000007  0.000010
---------------------------
Iteration 1, loss = 0.95633227
Iteration 2, loss = 0.90800165
Iteration 3, loss = 0.85930099
Iteration 4, loss = 0.81315008
Iteration 5, loss = 0.77767011
Iteration 6, loss = 0.75029890
Iteration 7, loss = 0.72867868
Iteration 8, loss = 0.70984405
Iteration 9, loss = 0.69284615
Iteration 10, loss = 0.67745974
Iteration 11, loss = 0.66369393
Iteration 12, loss = 0.65079638
Iteration 13, loss = 0.63815115
Iteration 14, loss = 0.62479042
Iteration 15, loss = 0.61194942
Iteration 16, loss = 0.59927277
Iteration 17, loss = 0.58521371
Iteration 18, loss = 0.57210814
Iteration 19, loss = 0.55996465
Iteration 20, loss = 0.54807067
Iteration 21, loss = 0.53650209
Iteration 22, loss = 0.52533811
Iteration 23, loss = 0.51471102
Iteration 24, loss = 0.50442994
Iteration 25, loss = 0.49484585
Iteration 26, loss = 0.48537805
Iteration 27, loss = 0.47662466
Iteration 28, loss = 0.46804487
Iteration 29, loss = 0.45946802
Iteration 30, loss = 0.45156293
Iteration 31, loss = 0.44394559
Iteration 32, loss = 0.43661760
Iteration 33, loss = 0.42951442
Iteration 34, loss = 0.42221744
Iteration 35, loss = 0.41564433
Iteration 36, loss = 0.40890426
Iteration 37, loss = 0.40264065
Iteration 38, loss = 0.39610546
Iteration 39, loss = 0.38993312
Iteration 40, loss = 0.38388316
Iteration 41, loss = 0.37800656
Iteration 42, loss = 0.37235001
Iteration 43, loss = 0.36619511
Iteration 44, loss = 0.36059324
Iteration 45, loss = 0.35476791
Iteration 46, loss = 0.34786957
Iteration 47, loss = 0.34153312
Iteration 48, loss = 0.33545813
Iteration 49, loss = 0.32930794
Iteration 50, loss = 0.32317794
Iteration 51, loss = 0.31638078
Iteration 52, loss = 0.31013908
Iteration 53, loss = 0.30393383
Iteration 54, loss = 0.29838288
Iteration 55, loss = 0.29297255
Iteration 56, loss = 0.28718813
Iteration 57, loss = 0.28172060
Iteration 58, loss = 0.27583385
Iteration 59, loss = 0.27005228
Iteration 60, loss = 0.26458281
Iteration 61, loss = 0.25943100
Iteration 62, loss = 0.25349329
Iteration 63, loss = 0.24806287
Iteration 64, loss = 0.24300222
Iteration 65, loss = 0.23774781
Iteration 66, loss = 0.23305851
Iteration 67, loss = 0.22823451
Iteration 68, loss = 0.22383759
Iteration 69, loss = 0.21977883
Iteration 70, loss = 0.21535604
Iteration 71, loss = 0.21166747
Iteration 72, loss = 0.20761852
Iteration 73, loss = 0.20394141
Iteration 74, loss = 0.20040514
Iteration 75, loss = 0.19708206
Iteration 76, loss = 0.19349267
Iteration 77, loss = 0.19027333
Iteration 78, loss = 0.18707219
Iteration 79, loss = 0.18402674
Iteration 80, loss = 0.18074351
Iteration 81, loss = 0.17816153
Iteration 82, loss = 0.17512573
Iteration 83, loss = 0.17238899
Iteration 84, loss = 0.16973877
Iteration 85, loss = 0.16697005
Iteration 86, loss = 0.16434632
Iteration 87, loss = 0.16195932
Iteration 88, loss = 0.15951366
Iteration 89, loss = 0.15725071
Iteration 90, loss = 0.15491615
Iteration 91, loss = 0.15290557
Iteration 92, loss = 0.15072733
Iteration 93, loss = 0.14883940
Iteration 94, loss = 0.14695900
Iteration 95, loss = 0.14484784
Iteration 96, loss = 0.14308214
Iteration 97, loss = 0.14122670
Iteration 98, loss = 0.13957297
Iteration 99, loss = 0.13775784
Iteration 100, loss = 0.13590304
Iteration 101, loss = 0.13438193
Iteration 102, loss = 0.13268324
Iteration 103, loss = 0.13098353
Iteration 104, loss = 0.12936521
Iteration 105, loss = 0.12791790
Iteration 106, loss = 0.12662701
Iteration 107, loss = 0.12498715
Iteration 108, loss = 0.12359448
Iteration 109, loss = 0.12249363
Iteration 110, loss = 0.12110071
Iteration 111, loss = 0.11981078
Iteration 112, loss = 0.11836256
Iteration 113, loss = 0.11714857
Iteration 114, loss = 0.11574617
Iteration 115, loss = 0.11434562
Iteration 116, loss = 0.11309169
Iteration 117, loss = 0.11172713
Iteration 118, loss = 0.11043709
Iteration 119, loss = 0.10930020
Iteration 120, loss = 0.10797385
Iteration 121, loss = 0.10698368
Iteration 122, loss = 0.10584113
Iteration 123, loss = 0.10474036
Iteration 124, loss = 0.10380168
Iteration 125, loss = 0.10255664
Iteration 126, loss = 0.10156837
Iteration 127, loss = 0.10070368
Iteration 128, loss = 0.09957178
Iteration 129, loss = 0.09860305
Iteration 130, loss = 0.09780964
Iteration 131, loss = 0.09681760
Iteration 132, loss = 0.09583616
Iteration 133, loss = 0.09508071
Iteration 134, loss = 0.09416457
Iteration 135, loss = 0.09321314
Iteration 136, loss = 0.09256206
Iteration 137, loss = 0.09184034
Iteration 138, loss = 0.09093598
Iteration 139, loss = 0.09011007
Iteration 140, loss = 0.08967234
Iteration 141, loss = 0.08887142
Iteration 142, loss = 0.08806606
Iteration 143, loss = 0.08736440
Iteration 144, loss = 0.08676293
Iteration 145, loss = 0.08606779
Iteration 146, loss = 0.08555681
Iteration 147, loss = 0.08473839
Iteration 148, loss = 0.08410474
Iteration 149, loss = 0.08344107
Iteration 150, loss = 0.08286863
Iteration 151, loss = 0.08220738
Iteration 152, loss = 0.08173567
Iteration 153, loss = 0.08113898
Iteration 154, loss = 0.08053362
Iteration 155, loss = 0.07993621
Iteration 156, loss = 0.07946923
Iteration 157, loss = 0.07885218
Iteration 158, loss = 0.07823091
Iteration 159, loss = 0.07786962
Iteration 160, loss = 0.07746597
Iteration 161, loss = 0.07675579
Iteration 162, loss = 0.07622176
Iteration 163, loss = 0.07587692
Iteration 164, loss = 0.07536789
Iteration 165, loss = 0.07485088
Iteration 166, loss = 0.07429429
Iteration 167, loss = 0.07382191
Iteration 168, loss = 0.07331210
Iteration 169, loss = 0.07279857
Iteration 170, loss = 0.07253759
Iteration 171, loss = 0.07198243
Iteration 172, loss = 0.07142384
Iteration 173, loss = 0.07111128
Iteration 174, loss = 0.07069893
Iteration 175, loss = 0.07013860
Iteration 176, loss = 0.06976115
Iteration 177, loss = 0.06923744
Iteration 178, loss = 0.06886139
Iteration 179, loss = 0.06856752
Iteration 180, loss = 0.06812569
Iteration 181, loss = 0.06779521
Iteration 182, loss = 0.06735216
Iteration 183, loss = 0.06686014
Iteration 184, loss = 0.06653185
Iteration 185, loss = 0.06621411
Iteration 186, loss = 0.06591546
Iteration 187, loss = 0.06548303
Iteration 188, loss = 0.06525067
Iteration 189, loss = 0.06473314
Iteration 190, loss = 0.06454455
Iteration 191, loss = 0.06409745
Iteration 192, loss = 0.06368881
Iteration 193, loss = 0.06348797
Iteration 194, loss = 0.06317595
Iteration 195, loss = 0.06273555
Iteration 196, loss = 0.06265236
Iteration 197, loss = 0.06215250
Iteration 198, loss = 0.06183776
Iteration 199, loss = 0.06145113
Iteration 200, loss = 0.06114145
Iteration 201, loss = 0.06096878
Iteration 202, loss = 0.06062923
Iteration 203, loss = 0.06027538
Iteration 204, loss = 0.06002334
Iteration 205, loss = 0.05973049
Iteration 206, loss = 0.05947466
Iteration 207, loss = 0.05915606
Iteration 208, loss = 0.05874766
Iteration 209, loss = 0.05864114
Iteration 210, loss = 0.05826253
Iteration 211, loss = 0.05793268
Iteration 212, loss = 0.05779692
Iteration 213, loss = 0.05743011
Iteration 214, loss = 0.05721951
Iteration 215, loss = 0.05698543
Iteration 216, loss = 0.05673126
Iteration 217, loss = 0.05648812
Iteration 218, loss = 0.05626781
Iteration 219, loss = 0.05595908
Iteration 220, loss = 0.05565794
Iteration 221, loss = 0.05541287
Iteration 222, loss = 0.05515807
Iteration 223, loss = 0.05495064
Iteration 224, loss = 0.05468909
Iteration 225, loss = 0.05443561
Iteration 226, loss = 0.05424260
Iteration 227, loss = 0.05399449
Iteration 228, loss = 0.05379619
Iteration 229, loss = 0.05365790
Iteration 230, loss = 0.05340179
Iteration 231, loss = 0.05313566
Iteration 232, loss = 0.05287018
Iteration 233, loss = 0.05261570
Iteration 234, loss = 0.05243164
Iteration 235, loss = 0.05212498
Iteration 236, loss = 0.05184360
Iteration 237, loss = 0.05163560
Iteration 238, loss = 0.05144388
Iteration 239, loss = 0.05124341
Iteration 240, loss = 0.05096182
Iteration 241, loss = 0.05082944
Iteration 242, loss = 0.05045252
Iteration 243, loss = 0.05041152
Iteration 244, loss = 0.05010375
Iteration 245, loss = 0.04994343
Iteration 246, loss = 0.04978505
Iteration 247, loss = 0.04949385
Iteration 248, loss = 0.04926800
Iteration 249, loss = 0.04918385
Iteration 250, loss = 0.04897857
Iteration 251, loss = 0.04873931
Iteration 252, loss = 0.04854403
Iteration 253, loss = 0.04835988
Iteration 254, loss = 0.04820827
Iteration 255, loss = 0.04809557
Iteration 256, loss = 0.04790502
Iteration 257, loss = 0.04767084
Iteration 258, loss = 0.04750192
Iteration 259, loss = 0.04737617
Iteration 260, loss = 0.04711255
Iteration 261, loss = 0.04697474
Iteration 262, loss = 0.04675612
Iteration 263, loss = 0.04655690
Iteration 264, loss = 0.04648630
Iteration 265, loss = 0.04626023
Iteration 266, loss = 0.04614926
Iteration 267, loss = 0.04598257
Iteration 268, loss = 0.04568912
Iteration 269, loss = 0.04567018
Iteration 270, loss = 0.04560093
Iteration 271, loss = 0.04548835
Iteration 272, loss = 0.04516538
Iteration 273, loss = 0.04502857
Iteration 274, loss = 0.04490064
Iteration 275, loss = 0.04475585
Iteration 276, loss = 0.04463777
Iteration 277, loss = 0.04445762
Iteration 278, loss = 0.04431224
Iteration 279, loss = 0.04425196
Iteration 280, loss = 0.04405462
Iteration 281, loss = 0.04379223
Iteration 282, loss = 0.04381320
Iteration 283, loss = 0.04338800
Iteration 284, loss = 0.04335969
Iteration 285, loss = 0.04305591
Iteration 286, loss = 0.04305370
Iteration 287, loss = 0.04277374
Iteration 288, loss = 0.04268327
Iteration 289, loss = 0.04250948
Iteration 290, loss = 0.04231249
Iteration 291, loss = 0.04231170
Iteration 292, loss = 0.04209035
Iteration 293, loss = 0.04194241
Iteration 294, loss = 0.04177476
Iteration 295, loss = 0.04174500
Iteration 296, loss = 0.04156010
Iteration 297, loss = 0.04148871
Iteration 298, loss = 0.04128376
Iteration 299, loss = 0.04111700
Iteration 300, loss = 0.04099103
Iteration 301, loss = 0.04089196
Iteration 302, loss = 0.04079276
Iteration 303, loss = 0.04051006
Iteration 304, loss = 0.04055557
Iteration 305, loss = 0.04042660
Iteration 306, loss = 0.04025504
Iteration 307, loss = 0.04012605
Iteration 308, loss = 0.04003016
Iteration 309, loss = 0.04002848
Iteration 310, loss = 0.03975573
Iteration 311, loss = 0.03968384
Iteration 312, loss = 0.03973362
Iteration 313, loss = 0.03945019
Iteration 314, loss = 0.03936315
Iteration 315, loss = 0.03925717
Iteration 316, loss = 0.03913293
Iteration 317, loss = 0.03912892
Iteration 318, loss = 0.03888059
Iteration 319, loss = 0.03889147
Iteration 320, loss = 0.03882322
Iteration 321, loss = 0.03864586
Iteration 322, loss = 0.03855206
Iteration 323, loss = 0.03847639
Iteration 324, loss = 0.03840040
Iteration 325, loss = 0.03815300
Iteration 326, loss = 0.03806923
Iteration 327, loss = 0.03797985
Iteration 328, loss = 0.03798110
Iteration 329, loss = 0.03781086
Iteration 330, loss = 0.03778302
Iteration 331, loss = 0.03764619
Iteration 332, loss = 0.03756237
Iteration 333, loss = 0.03740746
Iteration 334, loss = 0.03735038
Iteration 335, loss = 0.03725375
Iteration 336, loss = 0.03716473
Iteration 337, loss = 0.03710492
Iteration 338, loss = 0.03713651
Iteration 339, loss = 0.03686983
Iteration 340, loss = 0.03677867
Iteration 341, loss = 0.03688972
Iteration 342, loss = 0.03667567
Iteration 343, loss = 0.03660284
Iteration 344, loss = 0.03650826
Iteration 345, loss = 0.03660702
Iteration 346, loss = 0.03634038
Iteration 347, loss = 0.03626102
Iteration 348, loss = 0.03630896
Iteration 349, loss = 0.03607554
Iteration 350, loss = 0.03601451
Iteration 351, loss = 0.03594596
Iteration 352, loss = 0.03598048
Iteration 353, loss = 0.03583467
Iteration 354, loss = 0.03581955
Iteration 355, loss = 0.03577721
Iteration 356, loss = 0.03568007
Iteration 357, loss = 0.03558463
Iteration 358, loss = 0.03558606
Iteration 359, loss = 0.03548213
Iteration 360, loss = 0.03535097
Iteration 361, loss = 0.03542635
Iteration 362, loss = 0.03521586
Iteration 363, loss = 0.03520358
Iteration 364, loss = 0.03507908
Iteration 365, loss = 0.03504632
Iteration 366, loss = 0.03512427
Iteration 367, loss = 0.03488907
Iteration 368, loss = 0.03491032
Iteration 369, loss = 0.03488802
Iteration 370, loss = 0.03473807
Iteration 371, loss = 0.03472956
Iteration 372, loss = 0.03468798
Iteration 373, loss = 0.03459770
Iteration 374, loss = 0.03454946
Iteration 375, loss = 0.03446311
Iteration 376, loss = 0.03436507
Iteration 377, loss = 0.03435840
Iteration 378, loss = 0.03442482
Iteration 379, loss = 0.03415424
Iteration 380, loss = 0.03419850
Iteration 381, loss = 0.03418389
Iteration 382, loss = 0.03404185
Iteration 383, loss = 0.03395652
Iteration 384, loss = 0.03390252
Iteration 385, loss = 0.03389094
Iteration 386, loss = 0.03386968
Iteration 387, loss = 0.03373854
Iteration 388, loss = 0.03377558
Iteration 389, loss = 0.03364394
Iteration 390, loss = 0.03356782
Iteration 391, loss = 0.03351068
Iteration 392, loss = 0.03350032
Iteration 393, loss = 0.03355833
Iteration 394, loss = 0.03348628
Iteration 395, loss = 0.03335983
Iteration 396, loss = 0.03339495
Iteration 397, loss = 0.03319170
Iteration 398, loss = 0.03311930
Iteration 399, loss = 0.03325313
Iteration 400, loss = 0.03311145
Iteration 401, loss = 0.03308204
Iteration 402, loss = 0.03305315
Iteration 403, loss = 0.03294360
Iteration 404, loss = 0.03287448
Iteration 405, loss = 0.03289715
Iteration 406, loss = 0.03288152
Iteration 407, loss = 0.03284745
Iteration 408, loss = 0.03281462
Iteration 409, loss = 0.03272860
Iteration 410, loss = 0.03268691
Iteration 411, loss = 0.03257283
Iteration 412, loss = 0.03264205
Iteration 413, loss = 0.03241627
Iteration 414, loss = 0.03248570
Iteration 415, loss = 0.03238240
Iteration 416, loss = 0.03238197
Iteration 417, loss = 0.03234539
Iteration 418, loss = 0.03228464
Iteration 419, loss = 0.03223586
Iteration 420, loss = 0.03228994
Iteration 421, loss = 0.03209288
Iteration 422, loss = 0.03219711
Iteration 423, loss = 0.03213700
Iteration 424, loss = 0.03202652
Iteration 425, loss = 0.03213607
Iteration 426, loss = 0.03199006
Iteration 427, loss = 0.03202012
Iteration 428, loss = 0.03189622
Iteration 429, loss = 0.03182079
Iteration 430, loss = 0.03185245
Iteration 431, loss = 0.03182891
Iteration 432, loss = 0.03181818
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Neural
test_accuracy: 0.80
test_precision: 0.99
test_f1_score: 0.99
test_recall: 0.99
n_samples: 1025009
fit_time: 1209.18
Time: 20 min 9.184 s
Emissions: 2.78 g (CO2-equivalents)
Energy consumed: 14.33 Wh
---------------------------
